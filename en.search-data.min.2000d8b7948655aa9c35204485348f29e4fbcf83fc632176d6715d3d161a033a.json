[{"id":0,"href":"/project/docs/Entrega2/3Dbrush/","title":"(1) 3D Brush","section":"Entrega 2","content":" Entrega 2 - 3D Brush # Introducci√≥n # Se presentar√° un demo que ilustra una aplicaci√≥n de pintura en 3D que utiliza transformaciones de pantalla a mundo, similares a las empleadas en el Ray-casting, en la cual el usuario ser√° capaz de mover con los dedos de su mano izquierda el mundo y con los dedos de su mano derecha ser√° capaz de dibujar, todo esto a trav√©s de la c√°mara donde los rayos de luz virtuales se \u0026ldquo;emiten\u0026rdquo; o \u0026ldquo;trazan\u0026rdquo; en su camino desde el punto focal de una c√°mara a trav√©s de cada p√≠xel en el sensor de la c√°mara para determinar qu√© es visible a lo largo del rayo en el escena 3D.\nContexto # Ray Casting # El Ray casting es el m√°s b√°sico de muchos algoritmos de renderizaci√≥n de gr√°ficos por computadora que usan el algoritmo geom√©trico de ray tracing. Los algoritmos basados en ray tracing operan en orden de imagen para renderizar escenas tridimensionales a im√°genes bidimensionales. Los rayos geom√©tricos son trazados desde el ojo del observador (trazado hacia atr√°s) para calcular la radiancia que viaja hacia el observador en la direcci√≥n del rayo. La rapidez y simplicidad del trazado de los rayos provienen de computar el color de la luz sin trazar recursivamente rayos adicionales para obtener la radiancia incidente en el punto donde el rayo intercepta. Esto elimina la posibilidad de renderizar con exactitud las reflexiones, refracciones, y las sombras. Aun as√≠ todos estos elementos pueden ser simulados en un cierto grado, con el uso ingenioso de mapas de textura u otros m√©todos. La velocidad de c√≥mputo del ray casting lo convirti√≥ en un m√©todo pr√°ctico de renderizado para los primeros videojuegos de 3D con renderizaci√≥n de escenas en tiempo real.\n| Resultados # Ver C√≥digo /** * https://editor.p5js.org/pixelfelt/sketches/oS5CwSbM1 * Finger painting: A starter sketch demonstrating how to \u0026#34;finger paint\u0026#34; with Handsfree.js * * Step 1: Press the ‚ñ∂ Play Button on the top left of the p5 editor (or load this sketch in a browser) * Step 2: Click \u0026#34;Start Webcam\u0026#34; underneath the black canvas * Step 3: Wait a few moments for everything to start (the computer vision models are large) * Step 4: Pinch your fingers to paint üëå The left index pointer (big black circle) is an eraser and the left pinky clears the whole screen * * ---------- * * How it works (CTRL+F for \u0026#34;#n\u0026#34; to jump to that bit of code): * -- Add Handsfree.js to index.html * #1 Configure Handsfree.js * #2 Detect when fingers are pinched and then paint there * #3 Draw your hands landmarks onto the sketch * * ---------- * * Docs: https://handsfree.js.org (old) * Docs: https://handsfree.dev (newer but missing examples) * GitHub: https://github.com/midiblocks/handsfree * Twitter (me + handsfree.js): https://github.com/pixelfelt * Twitter (just handsfree.js): https://github.com/handsfreejs * * ---------- * * Ideas: * - Experiment with different input methods (like pinching to cycle through colors). Gestures are coming soon: https://handsfree.js.org/gesture/ * - If you need 3D (but with a limit of 1 hand), see here: https://handsfree.js.org/ref/model/handpose.html * - Try the \u0026#34;palm pointer\u0026#34; for a different approach to painting: https://handsfree.js.org/ref/plugin/palmPointers.html * - This can support up to 4 hands: https://handsfree.js.org/ref/model/hands.html#with-config */ // This will contain all of our lines paint = [] // Controles de las brocha let easycam; let state; let escorzo; let points; let record; let dx; let dy; let colorPi; let depth; let brush; // This is like pmouseX and pmouseY...but for every finger [pointer, middle, ring, pinky] let prevPointer = [ // Left hand [{x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}], // Right hand [{x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}] ] let posPointer = [ // Left hand [{x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}], // Right hand [{x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}, {x: 0, y: 0}] ] let PrevpointerCopy = prevPointer; // Landmark indexes for fingertips [pointer, middle, ring, pinky]...these are the same for both hands let fingertips = [8, 12, 16, 20] /** * Setup * - Configure handsfree (set which models, plugins, and gestures you want to use) * - Create start/stop buttons. It\u0026#39;s nice to always ask user for permission to start webcam :) */ function setup () { depth = 0; record = true; sketch = createCanvas(640, 480, WEBGL); colorPi = createColorPicker(\u0026#39;#ed225d\u0026#39;); colorPi.position(width - 70, 40); dx = sketch.width/2; dy = sketch.height / 2; points = [] // Colors for each fingertip colorMap = [ // Left fingertips [color(0, 0, 0), color(255, 0, 255), color(0, 0, 255), color(255, 255, 255)], // Right fingertips [color(255, 0, 0), color(0, 255, 0), color(0, 0, 255), color(255, 255, 0)] ] // #1 Turn on some models (hand tracking) and the show debugger // @see https://handsfree.js.org/#quickstart-workflow handsfree = new Handsfree({ showDebug: true, // Comment this out to hide the default webcam feed with landmarks hands: true }) handsfree.enablePlugins(\u0026#39;browser\u0026#39;) handsfree.plugin.pinchScroll.disable() // Add webcam buttons under the canvas // Handsfree.js comes with a bunch of classes to simplify hiding/showing things when things are loading // @see https://handsfree.js.org/ref/util/classes.html#started-loading-and-stopped-states buttonStart = createButton(\u0026#39;Start Webcam\u0026#39;) buttonStart.class(\u0026#39;handsfree-show-when-stopped\u0026#39;) buttonStart.class(\u0026#39;handsfree-hide-when-loading\u0026#39;) buttonStart.mousePressed(() =\u0026gt; handsfree.start()) // Create a \u0026#34;loading...\u0026#34; button buttonLoading = createButton(\u0026#39;...loading...\u0026#39;) buttonLoading.class(\u0026#39;handsfree-show-when-loading\u0026#39;) // Create a stop button buttonStop = createButton(\u0026#39;Stop Webcam\u0026#39;) buttonStop.class(\u0026#39;handsfree-show-when-started\u0026#39;) buttonStop.mousePressed(() =\u0026gt; handsfree.stop()) // easycam stuff let state = { distance: 250, // scalar center: [0, 0, 0], // vector rotation: [0, 0, 0, 1], // quaternion }; easycam = createEasyCam(); easycam.state_reset = state; // state to use on reset (double-click/tap) easycam.setState(state, 2000); // now animate to that state escorzo = true; perspective(); brush = sphereBrush; } /** * Main draw loop */ function draw () { background(0) fingerPaint() mousePaint() drawHands() update(); background(120); push(); strokeWeight(0.8); stroke(\u0026#39;magenta\u0026#39;); grid({ dotted: false }); pop(); axes(); for (const point of points) { push(); translate(point.worldPosition); brush(point); pop(); } // Revisar la rotacion en x y y , handsfree.on(\u0026#39;finger-pinched-0-0\u0026#39;, () =\u0026gt; { // Display the x and y of the right pinky if(handsfree.data.hands.curPinch[0][0].x != posPointer[0][0].x || handsfree.data.hands.curPinch[0][0].y != posPointer[0][0].y){ if(handsfree.data.hands.curPinch[0][0].x \u0026gt; posPointer[0][0].x){ easycam.rotateY(0.1); //console.log(\u0026#34;derecha\u0026#34;); }if(handsfree.data.hands.curPinch[0][0].x \u0026lt; posPointer[0][0].x){ easycam.rotateY(-0.1); //console.log(\u0026#34;izquierda\u0026#34;); }if(handsfree.data.hands.curPinch[0][0].y \u0026gt; posPointer[0][0].y){ easycam.rotateX(0.1); //console.log(\u0026#34;arriba\u0026#34;); }if(handsfree.data.hands.curPinch[0][0].y \u0026lt; posPointer[0][0].y){ easycam.rotateX(-0.1); //console.log(\u0026#34;abajo\u0026#34;); } } posPointer[0][0].x = handsfree.data.hands.curPinch[0][0].x; posPointer[0][0].y = handsfree.data.hands.curPinch[0][0].y; }) // revisar la rotacion en z handsfree.on(\u0026#39;finger-pinched-0-1\u0026#39;, () =\u0026gt; { // Display the x and y of the right pinky if(handsfree.data.hands.curPinch[0][1].y != posPointer[0][1].y){ if(handsfree.data.hands.curPinch[0][1].y \u0026gt; posPointer[0][1].y){ easycam.zoom(15); //console.log(\u0026#34;arriba\u0026#34;); }if(handsfree.data.hands.curPinch[0][1].y \u0026lt; posPointer[0][1].y){ easycam.zoom(-15); //console.log(\u0026#34;abajo\u0026#34;); } } posPointer[0][1].y = handsfree.data.hands.curPinch[0][1].y; }) } function update() { speed = constrain((dx + dy) / (2 * (width - height)), 0, 1); // Dibujar con un color handsfree.on(\u0026#39;finger-pinched-1-0\u0026#39;, () =\u0026gt; { // Display the x and y of the right pinky if(handsfree.data.hands.curPinch[1][0].x != posPointer[1][0].x || handsfree.data.hands.curPinch[1][0].y != posPointer[1][0].y){ if(handsfree.data.hands.curPinch[1][0].x \u0026gt; posPointer[1][0].x){ dx -= 10 ; }if(handsfree.data.hands.curPinch[1][0].x \u0026lt; posPointer[1][0].x){ dx += 10 ; }if(handsfree.data.hands.curPinch[1][0].y \u0026gt; posPointer[1][0].y){ dy += 10 ; }if(handsfree.data.hands.curPinch[1][0].y \u0026lt; posPointer[1][0].y){ dy -= 10; } } console.log(dx, \u0026#34; \u0026#34;, dy, \u0026#34; \u0026#34;, depth); posPointer[1][0].x = handsfree.data.hands.curPinch[1][0].x; posPointer[1][0].y = handsfree.data.hands.curPinch[1][0].y; }) if(handsfree.data.hands \u0026amp;\u0026amp; handsfree.data.hands.pinchState){ console.log(\u0026#34;esta con estados\u0026#34;); if (handsfree.data.hands.pinchState[0][2] == \u0026#39;held\u0026#39;) { console.log(\u0026#34;esta grabando\u0026#34;); points.push({ worldPosition: treeLocation([dx, dy, 0.5], { from: \u0026#39;SCREEN\u0026#39;, to: \u0026#39;WORLD\u0026#39; }), color: colorPi.color(), speed: speed }); } } } /** * #2 Finger paint * Since p5.js already has it\u0026#39;s own loop, we just check the data directly * @see https://handsfree.js.org/ref/plugin/pinchers.html */ // Whenever we pinch and move we\u0026#39;ll store those points as a set of [x1, y1, handIndex, fingerIndex, size] function fingerPaint () { let bounds = document.querySelector(\u0026#39;canvas\u0026#39;).getClientRects()[0] const hands = handsfree.data?.hands if (hands?.pinchState) { hands.pinchState.forEach((hand, handIndex) =\u0026gt; { hand.forEach((state, finger) =\u0026gt; { if (hands.landmarks?.[handIndex]?.[fingertips[finger]]) { let x = sketch.width - hands.landmarks[handIndex][fingertips[finger]].x * sketch.width let y = hands.landmarks[handIndex][fingertips[finger]].y * sketch.height // Start line on the spot that we pinched if (state === \u0026#39;start\u0026#39;) { prevPointer[handIndex][finger] = {x, y} // Add a line to the paint array } else if (state === \u0026#39;held\u0026#39;) { paint.push([ prevPointer[handIndex][finger].x, prevPointer[handIndex][finger].y, x, y, colorMap[handIndex][finger] ]) } // Set the last position prevPointer[handIndex][finger] = {x, y} } }) }) } // Clear everything if the left [0] pinky [3] is pinched if (hands?.pinchState \u0026amp;\u0026amp; hands.pinchState[0][3] === \u0026#39;released\u0026#39;) { paint = [] } // Draw Paint paint.forEach(p =\u0026gt; { push() fill(255) stroke(p[4]) strokeWeight(10) line(p[0], p[1], p[2], p[3]) pop(); }) } /** * Draw the mouse */ function mousePaint () { if (mouseIsPressed === true) { fill(colorMap[1][0]) stroke(colorMap[1][0]) strokeWeight(10) line(mouseX, mouseY, 0 , pmouseX, pmouseY, 0) } } /** * #3 Draw the hands into the P5 canvas * @see https://handsfree.js.org/ref/model/hands.html#data */ function drawHands () { const hands = handsfree.data?.hands // Bail if we don\u0026#39;t have anything to draw if (!hands?.landmarks) return // Draw keypoints hands.landmarks.forEach((hand, handIndex) =\u0026gt; { hand.forEach((landmark, landmarkIndex) =\u0026gt; { // Set color // @see https://handsfree.js.org/ref/model/hands.html#data if (colorMap[handIndex]) { switch (landmarkIndex) { case 8: fill(colorMap[handIndex][0]); break case 12: fill(colorMap[handIndex][1]); break case 16: fill(colorMap[handIndex][2]); break case 20: fill(colorMap[handIndex][3]); break default: fill(color(255, 255, 255)) } } // Set stroke if (handIndex === 0 \u0026amp;\u0026amp; landmarkIndex === 8) { stroke(color(255, 255, 255)) strokeWeight(5) circleSize = 40 } else { stroke(color(0, 0, 0)) strokeWeight(0) circleSize = 10 } circle( // Flip horizontally sketch.width - landmark.x * sketch.width, landmark.y * sketch.height, circleSize ) }) }) } function sphereBrush(point) { push(); noStroke(); // TODO parameterize sphere radius and / or // alpha channel according to gesture speed fill(point.color); sphere(1); pop(); } Conclusiones y trabajo futuro # Como se observa se puede mover el mundo con ayuda de la mano izquierda, gracias a la librer√≠a Handsfree.js la cual es capaz de detectar la posici√≥n de los dedos y simular la profundidad con la c√°mara web. Al ejecutar la c√°mara en la p√°gina web esta consume muchos recursos de la CPU ya que hace software rendering en la pintura en 3D, y por otro lado, este tiene algunos problemas al reconocer los gestos, pero se ha podido dibujar y manipular el mundo.\nPara trabajos futuros se espera que se puedan hacer dibujos m√°s f√°cilmente y con diferentes texturas, adem√°s que se pueda optimizar el algoritmo o la manera de interpretar los gestos para un manejo m√°s f√°cil del mundo y tambi√©n una manera m√°s din√°mica para dibujar, este es un tema apasionante y tambi√©n con un gran futuro por delante ya que pueden existir varios usos importantes en todas las ramas del conocimiento en las que se puedan aplicar este tipo de algoritmos.\nRecursos # colaboradores de Wikipedia. (2022, 26 febrero). Ray casting. Wikipedia, la enciclopedia libre. https://es.wikipedia.org/wiki/Ray_castingnpm: handsfree. (2021, 28 agosto).\nNpm. https://www.npmjs.com/package/handsfree\nWikipedia contributors. (2022, 10 mayo). Ray casting. Wikipedia. https://en.wikipedia.org/wiki/Ray_casting#History_of_ray_casting\n"},{"id":1,"href":"/project/docs/Entrega3/ShaderExer/","title":"(1) √ìscar","section":"Entrega 3","content":" Entrega Entrega 3 - Shaders # Coloring # Texturing - UV Visualization # Texturing - World # Texturing - Screen # "},{"id":2,"href":"/project/docs/Entrega3/gerson_codes/","title":"(2) Gerson Nicol√°s Pineda","section":"Entrega 3","content":" Entrega 3 - Shaders # Renderizaci√≥n por rasterizaci√≥n # Aplicaciones # "},{"id":3,"href":"/project/docs/Entrega2/Ray-Tracing/","title":"(2) Ray Tracing","section":"Entrega 2","content":" Entrega 2 - Ray Tracing # Ray tracing # Introducci√≥n # El ray tracing es una de las dos soluciones m√°s populares dadas para el renderizado de objetos de tres dimensiones en pantallas de s√≥lo dos. Esta t√©cnica permite la creaci√≥n de im√°genes que se asemejan a la realidad de una forma relativamente sencilla. A continuaci√≥n se muestra la historia de la t√©cnica, la teor√≠a que se encuentra detr√°s y una aplicaci√≥n que permite visualizar a rasgos generales como es el funcionamiento del ray tracing, es decir la realizaci√≥n de una prueba de concepto. De la revisi√≥n del marco te√≥rico es posible concluir que esta t√©cnica es cada vez m√°s usada debido a los adelantos computacionales y las nuevas necesidades que surgen en la computaci√≥n gr√°fica.\nContexto # Historia # Con la finalidad de obtener en las pantallas objetos cada vez m√°s parecidos a los que se encuentran en la realidad se comenz√≥ a intentar producir los gr√°ficos con conceptos de f√≠sica, en donde era posible ver mediante los rayos de luz que llegaban al observador, este proceso en el cual se segu√≠a un rayo de luz que sal√≠a de una fuente de luz hasta llegar al observador para formar una imagen fue llamado Ray Tracing. En el momento en el que se tuvo el concepto y la idea de realizar este tipo de procesado de las im√°genes no era posible debido a su costo computacional (1960). Con el paso del tiempo se ha podido mejorar los algoritmos y adem√°s se tiene mayor capacidad de c√≥mputo con lo cual se ha llegado al tratamiento de aspectos cada vez m√°s complejos como lo son los reflejos, las sombras o el efecto que se produce en las im√°genes en movimiento (Glassner, 2019).\nFuncionamiento # Como ya se hab√≠a se√±alado anteriormente, el Ray Tracing se inspira en la f√≠sica de la visi√≥n, por lo tanto se parte de un punto focal que vendr√≠a a ser el ojo del observador al cual llegan los rayos de luz, adem√°s se tiene un plano de proyecci√≥n que es en el cual se ver√° la imagen, este es el que ser√° emulado por la pantalla en donde se est√© observando la imagen, este plano usualmente se subdivide en la cantidad de pixeles que tiene la pantalla. Si se parte de una fuente de luz esta rebota en los objetos y pasa directamente hacia el plano de proyecci√≥n y despu√©s impacta contra el punto focal tal y como se muestra en la siguiente imagen retomada de Redmon(2011):\nSin embargo, este enfoque aunque correcto no es muy pr√°ctico, esto debido a la cantidad de rayos de luz o fotones que se encuentran rondado en una imagen o en el mundo real, ademas solo son necesarios para la computaci√≥n gr√°fica los rayos que llegan al punto focal, por lo que se cambia el enfoque. Para el ray tracing se parte del punto focal que hace pasar los rayos de luz por cada uno de los puntos del plano de proyecci√≥n se renderiza la imagen a partir de algunas propiedades que se dan al impactar el rayo contra los objetos que se encuentran en la imagen, entonces el algoritmo se basa en encontrar el punto de intersecci√≥n m√°s cercano del rayo con el objeto de la imagen y renderizar en el plano de proyecci√≥n alguna imagen de acuerdo con la distancia de la intersecci√≥n al punto focal, en donde adem√°s se pueden tener en cuenta otros factores como el color o la capacidad de absorber luz que tiene el objeto sobre el cual impacta el rayo (Redmon, 2011).\nResultados # Con el fin de demostrar el concepto se puede realizar el trazado de los rayos para renderizar una esfera, para esto es necesario primero determinar el sistema de referencia y con esto poder renderizar sobre el plano de proyecci√≥n es decir determinar la relaci√≥n entre el plano y los p√≠xeles de la pantalla, adem√°s cuando el rayo impacte sobre el rayo es necesario saber que color sera el que se dibujara sobre le plano y por √∫ltimo colorear el pixel correspondiente, acontinuaci√≥n se muestran los desarrollados realizados por Gambetta (2021). Para comenzar se realiza la relaci√≥n entre rayos de luz y los p√≠xeles:\n\\[V_x = C_x \\frac{V_w}{C_w}\\] \\[V_y = C_y \\frac{V_h}{C_h}\\] En donde C hace refenrencia al tama√±o del plano que se esta usando y V es el tama√±o de la pantalla real sobre la cual se proyectara la imagen. Con esta relaci√≥n se puede determinar un pixel por cada una de los puntos en le plano sobre el cual se dibujara lo que es interceptado por los rayos. El siguiente paso es disparar un rayo por cada uno de los puntos en el plano desde el punto focal, para esto se utiliza la ecuaci√≥n param√©trica de la recta en tres dimensiones y adem√°s se parte de un punto O que ser√° el origen o el punto focal y se dispara hacia cada uno de los puntos de V es decir del plano sobre el cual se dibujara. A continuaci√≥n se mostrar√° la ecuaci√≥n de la recta en donde se puede representar cualquier punto P y ademas t se√±ala el recorrido del rayo : \\[P = \\vec{O} \u0026#43; t(\\vec{V} - \\vec{O})\\] Como se renderizaran esferas, es necesario definir estas de forma matem√°tica, por lo que se parte de que las esferas tiene un centro y un radio y al tener se puede definir que la relaci√≥n que existe entre el punto del plano de proyecci√≥n y el centro de la esfera, al juntar esta ecuaci√≥n con la del rayo y al reemplazar V - O por el vector D, es decir la direcci√≥n del vector, se obtiene la siguiente ecuaci√≥n en donde el simbolo \u0026lt;\u0026gt; representa el producto punto: \\[\u0026lt;\\vec{O} \u0026#43; t \\vec{D} - \\vec{C} , \\vec{O} \u0026#43; t \\vec{D} - \\vec{C} \u0026gt; = r^2\\] Al desarrollar estas ecuaciones es posible llegar a que la distancia entre los puntos y el punto focal se puede resolver mediante ecuaciones cuadr√°ticas como se ilustra en la siguiente figura:\nCon la finalidad de realizar un renderizado correcto de la luz tambi√©n es necesario tener en cuenta la luz ambiental o la luz especular que llega a un punto P dado, para lo cual cada uno de los puntos requieren de un vector normal N, este sera necesario para computar a intensidad de la luz en cada uno de los puntos: \\[I_P = I_A \u0026#43; \\sum_{i=1}^n I_i \\frac{\u0026lt; \\vec{N}, \\vec{L_i}\u0026gt;}{| \\vec{N}| | \\vec{L_i}|}\\] Ademas es necesarip definir que la esfera sobre la cual se refleja la luz tiene un vector normal para cada uno de los puntos, y este se puede hallar con el punto donde se quiere hallar la normal y el centro de la esfera: \\[\\vec{N} = \\frac{P-C}{|P-C|}\\] Con todos estos elementos es posible presentar el resultado final en el cual se renderizan algunas esferas a las cuales la luz les llega desde un punto en la parte derecha de la pantalla:\nVer C√≥digo let canvas; let canvas_context; let canvas_pitch; let canvas_buffer; let viewport_size ; let projection_plane_z; let camera_position; let background_color; let spheres; let lights; let dy; let incre ; let liSliderPInt; let liSliderPX; let liSliderPY; function setup(){ canvas = createCanvas(600,600); //console.log(drawingContext); canvas_buffer = drawingContext.getImageData(0,0,canvas.width, canvas.height); canvas_pitch = canvas.width*4; viewport_size = 1; projection_plane_z = 1; camera_position = [0, 0, 0]; background_color = [255, 255, 255]; dy = 0.2; incre = true; liSliderPInt = createSlider(0, 1, 0.2, 0.05); liSliderPInt.position(10, 10); liSliderPInt.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); liSliderPX = createSlider(-5, 5, 0, 0.5); liSliderPX.position(10, 30); liSliderPX.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); liSliderPY = createSlider(-5, 5, 0, 0.5); liSliderPY.position(10, 50); liSliderPY.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); spheres = [new Sphere([0, dy, 3], 0.2, [255, 0, 0])]; lights = [ new Light(Light.AMBIENT,0.2), new Light(Light.POINT, liSliderPInt.value(), [liSliderPX.value(), 1, 0]), new Light(Light.DIRECTIONAL, 1, [3, 4 , 4]) ]; } function PutPixel(x, y, color) { var x = canvas.width/2 + x; var y = canvas.height/2 - y - 1; if (x \u0026lt; 0 || x \u0026gt;= canvas.width || y \u0026lt; 0 || y \u0026gt;= canvas.height) { return; } var offset = 4*x + canvas_pitch*y; canvas_buffer.data[offset++] = color[0]; canvas_buffer.data[offset++] = color[1]; canvas_buffer.data[offset++] = color[2]; canvas_buffer.data[offset++] = 255; } function UpdateCanvas() { drawingContext.putImageData(canvas_buffer, 0, 0); } function DotProduct(v1, v2) { return v1[0]*v2[0] + v1[1]*v2[1] + v1[2]*v2[2]; } function Length(vec) { return Math.sqrt(DotProduct(vec, vec)); } function Multiply(k, vec) { return [k*vec[0], k*vec[1], k*vec[2]]; } function Add(v1, v2) { return [v1[0] + v2[0], v1[1] + v2[1], v1[2] + v2[2]]; } function Subtract(v1, v2) { return [v1[0] - v2[0], v1[1] - v2[1], v1[2] - v2[2]]; } function Clamp(vec) { return [Math.min(255, Math.max(0, vec[0])), Math.min(255, Math.max(0, vec[1])), Math.min(255, Math.max(0, vec[2]))]; } function Sphere(center, radius, color) { this.center = center; this.radius = radius; this.color = color; } function Light(ltype, intensity, position) { this.ltype = ltype; this.intensity = intensity; this.position = position; } Light.AMBIENT = 0; Light.POINT = 1; Light.DIRECTIONAL = 2; function CanvasToViewport(p2d) { return [p2d[0] * viewport_size / canvas.width, p2d[1] * viewport_size / canvas.height, projection_plane_z]; } function IntersectRaySphere(origin, direction, sphere) { var oc = Subtract(origin, sphere.center); var k1 = DotProduct(direction, direction); var k2 = 2*DotProduct(oc, direction); var k3 = DotProduct(oc, oc) - sphere.radius*sphere.radius; var discriminant = k2*k2 - 4*k1*k3; if (discriminant \u0026lt; 0) { return [Infinity, Infinity]; } var t1 = (-k2 + Math.sqrt(discriminant)) / (2*k1); var t2 = (-k2 - Math.sqrt(discriminant)) / (2*k1); return [t1, t2]; } function ComputeLighting(point, normal) { var intensity = 0; var length_n = Length(normal); // Should be 1.0, but just in case... for (var i = 0; i \u0026lt; lights.length; i++) { var light = lights[i]; if (light.ltype == Light.AMBIENT) { intensity += light.intensity; } else { var vec_l; if (light.ltype == Light.POINT) { vec_l = Subtract(light.position, point); } else { // Light.DIRECTIONAL vec_l = light.position; } var n_dot_l = DotProduct(normal, vec_l); if (n_dot_l \u0026gt; 0) { intensity += light.intensity * n_dot_l / (length_n * Length(vec_l)); } } } return intensity; } function TraceRay(origin, direction, min_t, max_t) { var closest_t = Infinity; var closest_sphere = null; for (var i = 0; i \u0026lt; spheres.length; i++) { var ts = IntersectRaySphere(origin, direction, spheres[i]); if (ts[0] \u0026lt; closest_t \u0026amp;\u0026amp; min_t \u0026lt; ts[0] \u0026amp;\u0026amp; ts[0] \u0026lt; max_t) { closest_t = ts[0]; closest_sphere = spheres[i]; } if (ts[1] \u0026lt; closest_t \u0026amp;\u0026amp; min_t \u0026lt; ts[1] \u0026amp;\u0026amp; ts[1] \u0026lt; max_t) { closest_t = ts[1]; closest_sphere = spheres[i]; } } if (closest_sphere == null) { return background_color; } var point = Add(origin, Multiply(closest_t, direction)); var normal = Subtract(point, closest_sphere.center); normal = Multiply(1.0 / Length(normal), normal); return Multiply(ComputeLighting(point, normal), closest_sphere.color); } function draw(){ for (var x = -canvas.width/2; x \u0026lt; canvas.width/2; x++) { for (var y = -canvas.height/2; y \u0026lt; canvas.height/2; y++) { var direction = CanvasToViewport([x, y]) var color = TraceRay(camera_position, direction, 1, Infinity); PutPixel(x, y, Clamp(color)); } } if(incre){ dy+=0.01; if(dy\u0026gt;=1){ incre=false; } }else{ dy-=0.01; if(dy\u0026lt;=-1){ incre=true; } } spheres[0].center[1] = dy; lights[1].intensity = liSliderPInt.value(); lights[1].position[0]= liSliderPX.value(); lights[1].position[1]= liSliderPY.value(); UpdateCanvas(); }; Ver C√≥digo let canvas; let canvas_context; let canvas_pitch; let canvas_buffer; let viewport_size ; let projection_plane_z; let camera_position; let background_color; let spheres; let lights; let dy; let incre ; let dx; let increx ; let liSliderPInt; let liSliderPX; let liSliderPY; function setup(){ canvas = createCanvas(600,600); //console.log(drawingContext); canvas_buffer = drawingContext.getImageData(0,0,canvas.width, canvas.height); canvas_pitch = canvas.width*4; viewport_size = 1; projection_plane_z = 1; camera_position = [0, 0, 0]; background_color = [255, 255, 255]; dy = 0.2; incre = true; dx = -0.6; increx = true; liSliderPInt = createSlider(0, 1, 0.2, 0.05); liSliderPInt.position(10, 10); liSliderPInt.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); liSliderPX = createSlider(-5, 5, 0, 0.5); liSliderPX.position(10, 30); liSliderPX.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); liSliderPY = createSlider(-5, 5, 0, 0.5); liSliderPY.position(10, 50); liSliderPY.style(\u0026#39;width\u0026#39;, \u0026#39;580px\u0026#39;); spheres = [new Sphere([0, dy, 3], 0.2, [255, 0, 0]), new Sphere([dx, -0.2 , 5], 0.8, [0, 255, 0]), new Sphere([0, 0, 7], 2, [0, 0, 255])]; lights = [ new Light(Light.AMBIENT,0.2), new Light(Light.POINT, liSliderPInt.value(), [liSliderPX.value(), 1, 0]), new Light(Light.DIRECTIONAL, 1, [3, 4 , 4]) ]; } function PutPixel(x, y, color) { var x = canvas.width/2 + x; var y = canvas.height/2 - y - 1; if (x \u0026lt; 0 || x \u0026gt;= canvas.width || y \u0026lt; 0 || y \u0026gt;= canvas.height) { return; } var offset = 4*x + canvas_pitch*y; canvas_buffer.data[offset++] = color[0]; canvas_buffer.data[offset++] = color[1]; canvas_buffer.data[offset++] = color[2]; canvas_buffer.data[offset++] = 255; } function UpdateCanvas() { drawingContext.putImageData(canvas_buffer, 0, 0); } function DotProduct(v1, v2) { return v1[0]*v2[0] + v1[1]*v2[1] + v1[2]*v2[2]; } function Length(vec) { return Math.sqrt(DotProduct(vec, vec)); } function Multiply(k, vec) { return [k*vec[0], k*vec[1], k*vec[2]]; } function Add(v1, v2) { return [v1[0] + v2[0], v1[1] + v2[1], v1[2] + v2[2]]; } function Subtract(v1, v2) { return [v1[0] - v2[0], v1[1] - v2[1], v1[2] - v2[2]]; } function Clamp(vec) { return [Math.min(255, Math.max(0, vec[0])), Math.min(255, Math.max(0, vec[1])), Math.min(255, Math.max(0, vec[2]))]; } function Sphere(center, radius, color) { this.center = center; this.radius = radius; this.color = color; } function Light(ltype, intensity, position) { this.ltype = ltype; this.intensity = intensity; this.position = position; } Light.AMBIENT = 0; Light.POINT = 1; Light.DIRECTIONAL = 2; function CanvasToViewport(p2d) { return [p2d[0] * viewport_size / canvas.width, p2d[1] * viewport_size / canvas.height, projection_plane_z]; } function IntersectRaySphere(origin, direction, sphere) { var oc = Subtract(origin, sphere.center); var k1 = DotProduct(direction, direction); var k2 = 2*DotProduct(oc, direction); var k3 = DotProduct(oc, oc) - sphere.radius*sphere.radius; var discriminant = k2*k2 - 4*k1*k3; if (discriminant \u0026lt; 0) { return [Infinity, Infinity]; } var t1 = (-k2 + Math.sqrt(discriminant)) / (2*k1); var t2 = (-k2 - Math.sqrt(discriminant)) / (2*k1); return [t1, t2]; } function ComputeLighting(point, normal) { var intensity = 0; var length_n = Length(normal); // Should be 1.0, but just in case... for (var i = 0; i \u0026lt; lights.length; i++) { var light = lights[i]; if (light.ltype == Light.AMBIENT) { intensity += light.intensity; } else { var vec_l; if (light.ltype == Light.POINT) { vec_l = Subtract(light.position, point); } else { // Light.DIRECTIONAL vec_l = light.position; } var n_dot_l = DotProduct(normal, vec_l); if (n_dot_l \u0026gt; 0) { intensity += light.intensity * n_dot_l / (length_n * Length(vec_l)); } } } return intensity; } function TraceRay(origin, direction, min_t, max_t) { var closest_t = Infinity; var closest_sphere = null; for (var i = 0; i \u0026lt; spheres.length; i++) { var ts = IntersectRaySphere(origin, direction, spheres[i]); if (ts[0] \u0026lt; closest_t \u0026amp;\u0026amp; min_t \u0026lt; ts[0] \u0026amp;\u0026amp; ts[0] \u0026lt; max_t) { closest_t = ts[0]; closest_sphere = spheres[i]; } if (ts[1] \u0026lt; closest_t \u0026amp;\u0026amp; min_t \u0026lt; ts[1] \u0026amp;\u0026amp; ts[1] \u0026lt; max_t) { closest_t = ts[1]; closest_sphere = spheres[i]; } } if (closest_sphere == null) { return background_color; } var point = Add(origin, Multiply(closest_t, direction)); var normal = Subtract(point, closest_sphere.center); normal = Multiply(1.0 / Length(normal), normal); return Multiply(ComputeLighting(point, normal), closest_sphere.color); } function draw(){ for (var x = -canvas.width/2; x \u0026lt; canvas.width/2; x++) { for (var y = -canvas.height/2; y \u0026lt; canvas.height/2; y++) { var direction = CanvasToViewport([x, y]) var color = TraceRay(camera_position, direction, 1, Infinity); PutPixel(x, y, Clamp(color)); } } if(incre){ dy+=0.01; if(dy\u0026gt;=1){ incre=false; } }else{ dy-=0.01; if(dy\u0026lt;=-1){ incre=true; } } if(increx){ dx+=0.01; if(dx\u0026gt;=1){ increx=false; } }else{ dx-=0.01; if(dx\u0026lt;=-1){ increx=true; } } spheres[0].center[1] = dy; lights[1].intensity = liSliderPInt.value(); lights[1].position[0]= liSliderPX.value(); lights[1].position[1]= liSliderPY.value(); spheres[1].center[0] = dx; UpdateCanvas(); }; Conclusiones y trabajo futuro # Se puede apreciar que es posible obtener unos efectos bastante realistas de luz cuando se aplica esta t√©cnica para el renderizado las im√°genes, igualmente se observa como aumenta el costo computacional al realizar las operaciones de ray casting lo cual puede llegar a ser un problema grave teniendo en cuenta que no se ha llegado a usar todos los efectos posibles como lo son por ejemplo los efectos de reflecci√≥n de la luz o el manejo de diferentes figuras geom√©tricas con diferentes materiales que puedan absorber mejor o peor la luz. Para el trabajo futuro se presenta la posibilidad de ampliar el programa incluyendo los efectos anteriormente mencionados pero tambi√©n otros a√∫n m√°s importantes como lo son las sombras.\nRecursos # Gambetta, Gabriel (2021). Computer Graphs from scratch. A programmer\u0026rsquo;s introduction to 3D rendering. No starch Press. San Francisco Glassner, Andrew (2019). An Introduction to Ray Tracing. Academic Press, retomado de https://www.realtimerendering.com/raytracing/An-Introduction-to-Ray-Tracing-The-Morgan-Kaufmann-Series-in-Computer-Graphics-.pdf Redmon, Joe (2011). Ray Tracing. Trabajo de Tes√≠s, Degree of Bachelor of Arts. Computer Science Department of Middlebury College, retomado de https://pjreddie.com/media/files/Redmon_Thesis.pdf "},{"id":4,"href":"/project/docs/Entrega3/Fernando/","title":"(3) Fernando Moreno Bautista","section":"Entrega 3","content":" Entrega Entrega 3 - Shaders # Introducci√≥n # A lo largo de la siguiente p√°gina se podr√° observar una serie de ejercicios en los cuales se aplican los shaders, esto en los siguientes aspectos: Coloring, Texturing, Image Processing y Procedural Texturing, ejercicios en los cuales se podr√°n ver los diferentes usos de estos shaders los cuales se hacen mediante hardware y al final veremos un peque√±o an√°lisis entre las diferencias que existen entre los ejercicios de la primera entrega, los cuales fueron mediante software y por ultimo una conclusi√≥n.\nContexto y Ejemplos # Coloring # ¬øC√≥mo se generan los colores dentro del triangulo? Cada v√©rtice puede contener no solo un conjunto de datos, sino que tambi√©n datos adicionales llamados atributos. Esos atributos pueden ser color del v√©rtice, vector normal, coordenada de textura, etc.\nEntonces, ¬øc√≥mo se generan esos colores del triangulo de abajo?, La respuesta es la interpolaci√≥n. Como podemos leer en Wikipedia, la interpolaci√≥n en matem√°ticas es un tipo de estimaci√≥n, un m√©todo que puede usarse para generar nuevos puntos de datos entre un conjunto discreto de puntos de datos conocidos. En el problema descrito se trata de generar colores interpolados dentro del tri√°ngulo renderizado. La forma de lograr esto es mediante el uso de coordenadas baric√©ntricas. En el ejemplo del triangulo de abajo podemos mover el mouse dentro del canva y veremos como se va interpolando los coleres a medida que el mose se va moviendo.\nColoring example # Texturing - UV Visualization # El mapeo de texturas es un m√©todo para definir detalles de alta frecuencia, textura superficial o informaci√≥n de color en un gr√°fico generado por computadora o en un modelo 3D. La t√©cnica original fue iniciada por Edwin Catmull en 1974 El mapeo de texturas originalmente se refer√≠a al mapeo difuso, un m√©todo que simplemente mapeaba p√≠xeles de una textura a una superficie 3D (\u0026ldquo;envolviendo\u0026rdquo; la imagen alrededor del objeto). En las √∫ltimas d√©cadas, el advenimiento del renderizado multipaso, multitexturizaci√≥n, mipmaps y mapeos m√°s complejos como el mapeo de altura, el mapeo de relieve, el mapeo normal, el mapeo de desplazamiento, el mapeo de reflexi√≥n, el mapeo especular, el mapeo de oclusi√≥n y muchas otras variaciones de la t√©cnica. (controlados por un sistema de materiales) han hecho posible simular casi fotorrealismo en tiempo real al reducir enormemente la cantidad de pol√≠gonos y c√°lculos de iluminaci√≥n necesarios para construir una escena 3D realista y funcional.\nLa textura se define en algo llamado \u0026ldquo;uv space\u0026rdquo;, que a su vez se puede mostrar de varias formas, como la que se muestra a continuaci√≥n utilizando los colores azul y rojo y en el segundo ejemplo se usa un shape diferente, en este caso es una elipse:\nTexturing - UV Visualization example # Texturing - 3D World # Tambi√©n se puede usar la Navegaci√≥n en la computadora y esta debera proporcionar al usuario informaci√≥n sobre la ubicaci√≥n y el movimiento. La navegaci√≥n es la m√°s utilizada por el usuario en grandes entornos 3D y presenta diferentes desaf√≠os como apoyar la conciencia espacial, brindar movimientos eficientes entre lugares distantes y hacer que la navegaci√≥n sea llevadera para que el usuario pueda concentrarse en tareas m√°s importantes. Estas t√©cnicas, tareas de navegaci√≥n, se pueden dividir en dos componentes: viaje y orientaci√≥n. Viajar implica moverse desde la ubicaci√≥n actual hasta el punto deseado. Wayfinding se refiere a encontrar y establecer rutas para llegar a un objetivo de viaje dentro del entorno virtual.\nTexturing - 3D Screen # Texturing - Texture sampling # En el modelo de \u0026ldquo;cono hexagonal\u0026rdquo; de HSV, el valor se define como el componente m√°s grande de un color, M. Esto coloca los tres colores primarios y tambi√©n todos los \u0026ldquo;colores secundarios\u0026rdquo; (cian, amarillo y magenta) en un plano con el blanco, formando una pir√°mide hexagonal fuera del cubo RGB.\nEn este caso es:\nV= max(R,G,B) = M # En el modelo HSL \u0026ldquo;bi-hexcone\u0026rdquo;, la luminosidad se define como el promedio de los componentes de color m√°s grandes y m√°s peque√±os, es decir, el rango medio de los componentes RGB. Esta definici√≥n tambi√©n pone los colores primarios y secundarios en un plano, pero un plano que pasa a medio camino entre el blanco y el negro.\nEn este caso es:\nL = mid(R,G,B) = 1/2 (M + m) # Conclusiones y trabajo futuro # sssssssss\n"},{"id":5,"href":"/project/docs/Entrega2/rendering/","title":"(3) Renderizaci√≥n en software","section":"Entrega 2","content":" Entrega 2 - Renderizaci√≥n en software # Renderizado por software # Se refiere a cuando el proceso de renderizado es realizado sin involucrar unidades de procesamiento gr√°fico ‚ÄìGPUs‚Äì, es decir, usando √∫nicamente CPUs. Este tipo de renderizado tiene la ventaja de no depender del rendimiento y memoria de la GPU, adem√°s de no tener que pagar por otro dispositivo. Sin embargo, estas ventajas frecuentemente suelen verse opacadas por las ventajas que ofrece el renderizado en GPU, como que este √∫ltimo es significativamente m√°s r√°pido debido a que las GPUs est√°n optimizadas para el procesamiento gr√°fico y la computaci√≥n paralela. [1]\nRenderizado # Es el √∫ltimo paso en el proceso de animaci√≥n. Es el proceso de generar im√°genes a partir de un modelo 2D o 3D, generalmente un modelo de estructura al√°mbrica, al cual se le a√±aden una serie de caracter√≠sticas por medio de un programa de computador [2]. Algunas de estas caracter√≠sticas son:\nSombreado plano: c√≥mo el color y el brillo var√≠an con la iluminaci√≥n. Mapeado de texturas: m√©todo para aplicar detalle a superficies. Mapeado topol√≥gico: m√©todo para simular rugosidad a peque√±a escala. Participaci√≥n de medio: c√≥mo la luz se aten√∫a cuando pasa a trav√©s de atm√≥sferas no claras en aire. Sombras: el efecto de obstruir la luz. Reflexi√≥n: apariencias de espejos. Transparencia u opacidad: transmisi√≥n de la luz a trav√©s de objetos s√≥lidos. Traslucidez: transmisi√≥n dispersa de la luz a trav√©s de objetos s√≥lidos. Refracci√≥n: doblado de la luz asociado con la transparencia. Difracci√≥n: doblado, dispersi√≥n e interferencia de la luz que pasa a trav√©s de un objeto o apertura que interrumpe el haz. Iluminaci√≥n indirecta: superficies iluminadas por la reflexi√≥n de otros objetos. C√°ustica: focalizaci√≥n de la luz al atravesar objetos transparentes que produce reflejos brillantes en otro objeto. Profundidad de campo: los objetos se ven borrosos cuando est√°n lejos del objeto en el foco. Desenfoque de movimiento: los objetos se ven borrosos debido al movimiento a altas velocidades o por el movimiento de la c√°mara. Cuando se realizan animaciones, una serie de im√°genes debe ser renderizadas para posteriormente ser puestas en secuencia mediante un programa.\nT√©cnicas de renderizado # Rasterizaci√≥n # Es el m√©todo de renderizaci√≥n usado por todas las tarjetas gr√°ficas. Usa tri√°ngulos y pol√≠gonos como primitivas cuando el renderizado p√≠xel por p√≠xel no se puede realizar por alg√∫n motivo. Se itera sobre cada primitiva y decide qu√© p√≠xeles se ver√°n afectados para modificarlos posteriormente.\nRay casting # Contrario a la rasterizaci√≥n, la geometr√≠a se modela p√≠xel por p√≠xel desde el punto de vista, como si se emitieran rayos desde all√≠. En el lugar en el que un objeto es intersecado se eval√∫a el color con alguna de las m√∫ltiples t√©cnicas. Esta t√©cnica es sensible a los artefactos, por lo que generalmente se promedian los valores de rayos en diferentes direcciones. Esta t√©cnica se diferencia del trazado de rayos en que este √∫ltimo s√≠ tiene en cuenta el rebote de los haces de luz.\nTrazado de rayos # Busca simular el comportamiento de la luz como part√≠cula, incluso llegando a simular el efecto de los espacio-tiempos relativistas. Generalmente se emiten varios haces de luz desde cada p√≠xel, siguiendo cada uno de sus rebotes mediante el uso de las leyes de la √≥ptica. Una vez cada haz encuentra otra fuente de iluminaci√≥n o alcanza cierto n√∫mero de rebotes, se eval√∫a el color del p√≠xel y de todos los rebotes.\nRenderizado neural # Usa redes neurales artificiales que usan m√©todos de renderizado basados en im√°genes que reconstruyen modelos 3D de modelos 2D.\nRadiosidad # Trata de simular la manera en que las superficies directamente iluminadas act√∫an como fuentes de iluminaci√≥n indirecta que iluminan otras superficies, lo que produce un sombreado m√°s realista y captura mejor los ambientes interiores. Su filosof√≠a se basa en que la iluminaci√≥n debe ser simulada como en realidad ocurre: disparando haces de luz en todas las direcciones y calculando todas sus trayectorias. [3]\nRepositorios # En este enlace es posible encontrar una lista de 14 repositorios acerca de rasterizadores en software. En ella se pueden encontrar renderizadores y rasterizadores, la mayor√≠a de estos implementados en lenguajes altamente eficientes como C, C++, Zig o Go.\nMesa # Es un programa open-source que implementa OpenGL, Vulkan y otras especificaciones de API gr√°ficas, y las traduce a controladores de componentes gr√°ficos de vendedores espec√≠ficos. En la actualidad, AMD e Intel desarrollan sus controladores basados en Mesa. [4]\nReferencias y bibliograf√≠a # [1] Glawion, A. (2022, 12 abril). CPU vs. GPU Rendering ‚Äì What‚Äôs the difference and which should you choose? CG Director. Recuperado 22 de mayo de 2022, de CPU vs. GPU Rendering ‚Äì What‚Äôs the difference and which should you choose?.\n[2] Wikipedia contributors. (2022, 18 abril). Rendering (computer graphics). Wikipedia. Recuperado 22 de mayo de 2022, de Rendering (computer graphics).\n[3] Wikipedia contributors. (2022b, mayo 4). Radiosity (computer graphics). Wikipedia. Recuperado 22 de mayo de 2022, de Radiosity (computer graphics).\n[4] Wikipedia contributors. (2022c, mayo 18). Mesa (computer graphics). Wikipedia. Recuperado 22 de mayo de 2022, de Mesa (computer graphics).\n"},{"id":6,"href":"/project/docs/ilusiones/","title":"Entrega 1","section":"Docs","content":" Entrega 1 - Ilusiones √≥pticas # Introducci√≥n # Se presentar√° el efecto estrobosc√≥pico en el cual se parte de im√°genes que se presentan de forma continua generando lo que parece ser un movimiento fluido, esto debido al efecto Phi, dentro de los posibles consecuencias que este efecto puede llegar a tener se encuentra una que relaciona el movimiento y la percepci√≥n de los colores en las pantallas de los computadores. Al seguir los pasos del trabajo realizado por Michael Bach fue posible reproducir el fen√≥meno. Para mostrar el trabajo realizado primero se har√° una breve contextualizaci√≥n desde la teor√≠a del fen√≥meno, luego este ser√° reproducido utilizando p5.js y por √∫ltimo se concluir√° acerca de lo hallado.\nContexto # Aliasing # Es un fen√≥meno que causa que diferentes se√±ales se vuelven indistinguibles (o alias de otras) cuando son muestreadas. Tambi√©n se refiere a cuando una se√±al reconstruida a trav√©s de muestras es distinta a su original continua. Cuando este fen√≥meno se presenta con muestras en el tiempo, como el efecto estrobosc√≥pico, se denomina aliasing temporal; cuando se presenta con muestras en el espacio, como con los patrones moir√©, se denomina aliasing espacial.\nCon una resoluci√≥n mayor no se produce aliasing Con una resoluci√≥n menor puede producirse aliasing Efecto estrobosc√≥pico # Es un efecto √≥ptico causado por el aliasing. Se produce cuando el movimiento rotacional o c√≠clico es representado por muestras cortas o instant√°neas a una tasa de muestreo cercana a la del periodo del movimiento. Este efecto fue crucial para el desarrollo de la animaci√≥n y del cine.\nFen√≥meno phi # Es una ilusi√≥n √≥ptica en la que se aparenta movimiento cuando en realidad se presentan una serie de im√°genes est√°ticas con una frecuencia relativamente alta.\nResultados # Ver C√≥digo // Adaptado del c√≥digo de Lindsey Piscitell [this](https://editor.p5js.org/LindseyPiscitell/sketches/SJgoswgp) // Variables var deg = 0; var sp; let cnv,g; var reset_button; //Dibuja el canvas function setup() { cnv = createCanvas(400, 400); cnv.mouseClicked(changeGray); sp =2; g=100; reset_button = createButton(\u0026#34;Reset\u0026#34;); // Posicion del boton reset_button.position(15, 50); reset_button.mouseClicked(reset); } function reset(){ sp = 0; } function draw() { background(g); textSize(38); fill(\u0026#39;white\u0026#39;); text(sp, 15, 35); translate (200,200); rotate (radians (deg)); ellipse(0,0,200,200); // Crea y rellena los arcos de color fill(\u0026#39;red\u0026#39;); arc(0, 0, 200, 200, 0, 180); fill(\u0026#39;blue\u0026#39;); arc(0, 0, 200, 200, 180, 0); fill(\u0026#39;green\u0026#39;); arc(0, 0, 200, 200, 0, 360); deg+=sp; print(sp); } // Esta funcion se activa cuando se da click en el canvas aumenta la velocidad function mouseClicked() { sp = sp + 10; } // Esta funcion se activa cuando se da click en el canvas cambia el color en escala de grises function changeGray() { g = random(0, 200); } Conclusiones y trabajo futuro # Como se observa la velocidad y tasa de actualizaci√≥n de p√≠xeles de la pantalla juega en contra cuando se desea tener precisi√≥n en ciertos aspectos de los est√≠mulos que se nos est√°n presentando mediante este medio, como lo es la sensaci√≥n de movimiento continuo y la mezcla de los colores causado por la velocidad a la cual se est√°n presentando los segmentos del c√≠rculo. La velocidad a la cual aparentemente se encuentra girando el c√≠rculo define si se van a ver los colores que originalmente se definieron o la combinaci√≥n de estos.\nRecursos # Raster-scan cathode-ray tubes for vision research\u0026ndash;limits of resolution in space, time and intensity, and some solutions. (1997). Spatial Vision, 10(4), 403-414. Wikipedia contributors. (2022, 21 marzo). Aliasing. Wikipedia. Recuperado 5 de abril de 2022, de https://en.wikipedia.org/wiki/Aliasing Wikipedia contributors. (2022a, febrero 27). Stroboscopic effect. Wikipedia. Recuperado 5 de abril de 2022, de https://en.wikipedia.org/wiki/Stroboscopic_effect Wikipedia contributors. (2022a, enero 31). Phi phenomenon. Wikipedia. Recuperado 5 de abril de 2022, de https://en.wikipedia.org/wiki/Phi_phenomenon "}]